{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing data\n",
    "data = pd.read_csv('data .csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:].values\n",
    "y = data.iloc[:, 1].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "y = labelencoder_X_1.fit_transform(y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 23:39:46.788244  8280 training.py:686] The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "512/512 [==============================] - 0s 736us/sample - loss: 0.6660 - accuracy: 0.5488\n",
      "Epoch 2/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.5791 - accuracy: 0.6875\n",
      "Epoch 3/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.5253 - accuracy: 0.7656\n",
      "Epoch 4/150\n",
      "512/512 [==============================] - 0s 58us/sample - loss: 0.4625 - accuracy: 0.8379\n",
      "Epoch 5/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.4137 - accuracy: 0.8633\n",
      "Epoch 6/150\n",
      "512/512 [==============================] - 0s 50us/sample - loss: 0.3783 - accuracy: 0.8867\n",
      "Epoch 7/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.3345 - accuracy: 0.8906\n",
      "Epoch 8/150\n",
      "512/512 [==============================] - 0s 58us/sample - loss: 0.3072 - accuracy: 0.8984\n",
      "Epoch 9/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.2708 - accuracy: 0.9160\n",
      "Epoch 10/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.2708 - accuracy: 0.9141\n",
      "Epoch 11/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.2415 - accuracy: 0.9160\n",
      "Epoch 12/150\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 0.2325 - accuracy: 0.9297\n",
      "Epoch 13/150\n",
      "512/512 [==============================] - 0s 60us/sample - loss: 0.2108 - accuracy: 0.9316\n",
      "Epoch 14/150\n",
      "512/512 [==============================] - 0s 62us/sample - loss: 0.2018 - accuracy: 0.9395\n",
      "Epoch 15/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.1784 - accuracy: 0.9434\n",
      "Epoch 16/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.1807 - accuracy: 0.9414\n",
      "Epoch 17/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.1687 - accuracy: 0.9648\n",
      "Epoch 18/150\n",
      "512/512 [==============================] - 0s 50us/sample - loss: 0.1645 - accuracy: 0.9609\n",
      "Epoch 19/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.1795 - accuracy: 0.9551\n",
      "Epoch 20/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.1564 - accuracy: 0.9531\n",
      "Epoch 21/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.1383 - accuracy: 0.9590\n",
      "Epoch 22/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.1482 - accuracy: 0.9512\n",
      "Epoch 23/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.1387 - accuracy: 0.9570\n",
      "Epoch 24/150\n",
      "512/512 [==============================] - 0s 57us/sample - loss: 0.1446 - accuracy: 0.9609\n",
      "Epoch 25/150\n",
      "512/512 [==============================] - 0s 44us/sample - loss: 0.1365 - accuracy: 0.9609\n",
      "Epoch 26/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.1248 - accuracy: 0.9629\n",
      "Epoch 27/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.1128 - accuracy: 0.9746\n",
      "Epoch 28/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.1316 - accuracy: 0.9629\n",
      "Epoch 29/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.1267 - accuracy: 0.9688\n",
      "Epoch 30/150\n",
      "512/512 [==============================] - 0s 38us/sample - loss: 0.1106 - accuracy: 0.9668\n",
      "Epoch 31/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.1107 - accuracy: 0.9727\n",
      "Epoch 32/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.1194 - accuracy: 0.9648\n",
      "Epoch 33/150\n",
      "512/512 [==============================] - 0s 52us/sample - loss: 0.1070 - accuracy: 0.9707\n",
      "Epoch 34/150\n",
      "512/512 [==============================] - 0s 70us/sample - loss: 0.1213 - accuracy: 0.9629\n",
      "Epoch 35/150\n",
      "512/512 [==============================] - 0s 50us/sample - loss: 0.1322 - accuracy: 0.9688\n",
      "Epoch 36/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.1108 - accuracy: 0.9766\n",
      "Epoch 37/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.1150 - accuracy: 0.9746\n",
      "Epoch 38/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.1223 - accuracy: 0.9688\n",
      "Epoch 39/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.1077 - accuracy: 0.9746\n",
      "Epoch 40/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.1143 - accuracy: 0.9746\n",
      "Epoch 41/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.1003 - accuracy: 0.9785\n",
      "Epoch 42/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0998 - accuracy: 0.9824\n",
      "Epoch 43/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.1134 - accuracy: 0.9785\n",
      "Epoch 44/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.1157 - accuracy: 0.9707\n",
      "Epoch 45/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.1049 - accuracy: 0.9824\n",
      "Epoch 46/150\n",
      "512/512 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.98 - 0s 41us/sample - loss: 0.0951 - accuracy: 0.9785\n",
      "Epoch 47/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0987 - accuracy: 0.9688\n",
      "Epoch 48/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0979 - accuracy: 0.9824\n",
      "Epoch 49/150\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 0.1023 - accuracy: 0.9824\n",
      "Epoch 50/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0937 - accuracy: 0.9746\n",
      "Epoch 51/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0868 - accuracy: 0.9805\n",
      "Epoch 52/150\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 0.0808 - accuracy: 0.9766\n",
      "Epoch 53/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0984 - accuracy: 0.9824\n",
      "Epoch 54/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.0824 - accuracy: 0.9844\n",
      "Epoch 55/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0841 - accuracy: 0.9883\n",
      "Epoch 56/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.1054 - accuracy: 0.9805\n",
      "Epoch 57/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0791 - accuracy: 0.9844\n",
      "Epoch 58/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0733 - accuracy: 0.9805\n",
      "Epoch 59/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.1121 - accuracy: 0.9785\n",
      "Epoch 60/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0961 - accuracy: 0.9805\n",
      "Epoch 61/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0935 - accuracy: 0.9805\n",
      "Epoch 62/150\n",
      "512/512 [==============================] - 0s 35us/sample - loss: 0.0927 - accuracy: 0.9805\n",
      "Epoch 63/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0692 - accuracy: 0.9863\n",
      "Epoch 64/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0829 - accuracy: 0.9863\n",
      "Epoch 65/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0816 - accuracy: 0.9883\n",
      "Epoch 66/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0681 - accuracy: 0.9824\n",
      "Epoch 67/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0809 - accuracy: 0.9883\n",
      "Epoch 68/150\n",
      "512/512 [==============================] - 0s 64us/sample - loss: 0.0892 - accuracy: 0.9785\n",
      "Epoch 69/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0918 - accuracy: 0.9863\n",
      "Epoch 70/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0750 - accuracy: 0.9883\n",
      "Epoch 71/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.0875 - accuracy: 0.9824\n",
      "Epoch 72/150\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 0.0776 - accuracy: 0.9863\n",
      "Epoch 73/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0885 - accuracy: 0.9902\n",
      "Epoch 74/150\n",
      "512/512 [==============================] - 0s 35us/sample - loss: 0.0759 - accuracy: 0.9863\n",
      "Epoch 75/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0830 - accuracy: 0.9902\n",
      "Epoch 76/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0766 - accuracy: 0.9902\n",
      "Epoch 77/150\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 0.0705 - accuracy: 0.9844\n",
      "Epoch 78/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 51us/sample - loss: 0.0580 - accuracy: 0.9902\n",
      "Epoch 79/150\n",
      "512/512 [==============================] - 0s 46us/sample - loss: 0.0764 - accuracy: 0.9883\n",
      "Epoch 80/150\n",
      "512/512 [==============================] - 0s 35us/sample - loss: 0.0757 - accuracy: 0.9902\n",
      "Epoch 81/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0678 - accuracy: 0.9883\n",
      "Epoch 82/150\n",
      "512/512 [==============================] - 0s 36us/sample - loss: 0.0719 - accuracy: 0.9902\n",
      "Epoch 83/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0798 - accuracy: 0.9902\n",
      "Epoch 84/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0776 - accuracy: 0.9883\n",
      "Epoch 85/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0825 - accuracy: 0.9883\n",
      "Epoch 86/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0776 - accuracy: 0.9902\n",
      "Epoch 87/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0813 - accuracy: 0.9863\n",
      "Epoch 88/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.0773 - accuracy: 0.9863\n",
      "Epoch 89/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0749 - accuracy: 0.9824\n",
      "Epoch 90/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0806 - accuracy: 0.9902\n",
      "Epoch 91/150\n",
      "512/512 [==============================] - 0s 44us/sample - loss: 0.0830 - accuracy: 0.9883\n",
      "Epoch 92/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0810 - accuracy: 0.9844\n",
      "Epoch 93/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0844 - accuracy: 0.9863\n",
      "Epoch 94/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0916 - accuracy: 0.9863\n",
      "Epoch 95/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0696 - accuracy: 0.9902\n",
      "Epoch 96/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0761 - accuracy: 0.9863\n",
      "Epoch 97/150\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 0.0865 - accuracy: 0.9844\n",
      "Epoch 98/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0779 - accuracy: 0.9883\n",
      "Epoch 99/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0616 - accuracy: 0.9863\n",
      "Epoch 100/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.0669 - accuracy: 0.9883\n",
      "Epoch 101/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.0693 - accuracy: 0.9883\n",
      "Epoch 102/150\n",
      "512/512 [==============================] - 0s 60us/sample - loss: 0.0621 - accuracy: 0.9844\n",
      "Epoch 103/150\n",
      "512/512 [==============================] - 0s 65us/sample - loss: 0.0709 - accuracy: 0.9883\n",
      "Epoch 104/150\n",
      "512/512 [==============================] - 0s 48us/sample - loss: 0.0681 - accuracy: 0.9902\n",
      "Epoch 105/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.0654 - accuracy: 0.9922\n",
      "Epoch 106/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.0716 - accuracy: 0.9883\n",
      "Epoch 107/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.0778 - accuracy: 0.9902\n",
      "Epoch 108/150\n",
      "512/512 [==============================] - 0s 54us/sample - loss: 0.0592 - accuracy: 0.9883\n",
      "Epoch 109/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0803 - accuracy: 0.9883\n",
      "Epoch 110/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0650 - accuracy: 0.9922\n",
      "Epoch 111/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0607 - accuracy: 0.9883\n",
      "Epoch 112/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0569 - accuracy: 0.9883\n",
      "Epoch 113/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0850 - accuracy: 0.9844\n",
      "Epoch 114/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0614 - accuracy: 0.9863\n",
      "Epoch 115/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0731 - accuracy: 0.9883\n",
      "Epoch 116/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0641 - accuracy: 0.9883\n",
      "Epoch 117/150\n",
      "512/512 [==============================] - 0s 46us/sample - loss: 0.0720 - accuracy: 0.9883\n",
      "Epoch 118/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0644 - accuracy: 0.9863\n",
      "Epoch 119/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0764 - accuracy: 0.9863\n",
      "Epoch 120/150\n",
      "512/512 [==============================] - 0s 40us/sample - loss: 0.0554 - accuracy: 0.9922\n",
      "Epoch 121/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0757 - accuracy: 0.9883\n",
      "Epoch 122/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0786 - accuracy: 0.9922\n",
      "Epoch 123/150\n",
      "512/512 [==============================] - 0s 33us/sample - loss: 0.0709 - accuracy: 0.9941\n",
      "Epoch 124/150\n",
      "512/512 [==============================] - 0s 53us/sample - loss: 0.0705 - accuracy: 0.9941\n",
      "Epoch 125/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0572 - accuracy: 0.9922\n",
      "Epoch 126/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0665 - accuracy: 0.9902\n",
      "Epoch 127/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0598 - accuracy: 0.9902\n",
      "Epoch 128/150\n",
      "512/512 [==============================] - 0s 35us/sample - loss: 0.0658 - accuracy: 0.9902\n",
      "Epoch 129/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.0735 - accuracy: 0.9922\n",
      "Epoch 130/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0664 - accuracy: 0.9941\n",
      "Epoch 131/150\n",
      "512/512 [==============================] - 0s 47us/sample - loss: 0.0626 - accuracy: 0.9902\n",
      "Epoch 132/150\n",
      "512/512 [==============================] - 0s 43us/sample - loss: 0.0674 - accuracy: 0.9902\n",
      "Epoch 133/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0564 - accuracy: 0.9941\n",
      "Epoch 134/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0641 - accuracy: 0.9922\n",
      "Epoch 135/150\n",
      "512/512 [==============================] - 0s 45us/sample - loss: 0.0550 - accuracy: 0.9941\n",
      "Epoch 136/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0651 - accuracy: 0.9941\n",
      "Epoch 137/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0799 - accuracy: 0.9863\n",
      "Epoch 138/150\n",
      "512/512 [==============================] - 0s 42us/sample - loss: 0.0644 - accuracy: 0.9941\n",
      "Epoch 139/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0707 - accuracy: 0.9922\n",
      "Epoch 140/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0585 - accuracy: 0.9902\n",
      "Epoch 141/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0667 - accuracy: 0.9902\n",
      "Epoch 142/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0589 - accuracy: 0.9902\n",
      "Epoch 143/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0656 - accuracy: 0.9922\n",
      "Epoch 144/150\n",
      "512/512 [==============================] - 0s 56us/sample - loss: 0.0628 - accuracy: 0.9922\n",
      "Epoch 145/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0620 - accuracy: 0.9902\n",
      "Epoch 146/150\n",
      "512/512 [==============================] - 0s 41us/sample - loss: 0.0571 - accuracy: 0.9902\n",
      "Epoch 147/150\n",
      "512/512 [==============================] - 0s 39us/sample - loss: 0.0551 - accuracy: 0.9922\n",
      "Epoch 148/150\n",
      "512/512 [==============================] - 0s 51us/sample - loss: 0.0595 - accuracy: 0.9902\n",
      "Epoch 149/150\n",
      "512/512 [==============================] - 0s 37us/sample - loss: 0.0475 - accuracy: 0.9883\n",
      "Epoch 150/150\n",
      "512/512 [==============================] - 0s 49us/sample - loss: 0.0539 - accuracy: 0.9902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae30a54e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(26, activation='relu', input_dim=30))\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(0.1))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(13,  activation='relu'))\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(6, activation='relu'))\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(0.1))\n",
    "classifier.add(Dense(1,  activation='sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=100, nb_epoch=150,shuffle=True)\n",
    "# Long scroll ahead but worth\n",
    "# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        35\n",
      "           1       0.96      1.00      0.98        22\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        57\n",
      "   macro avg       0.98      0.99      0.98        57\n",
      "weighted avg       0.98      0.98      0.98        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy is 98.24561403508771%\n"
     ]
    }
   ],
   "source": [
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
